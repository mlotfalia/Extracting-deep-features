{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip3 install --upgrade tensorflow-gpu --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "text_train_des, text_test_des, y_train_des, y_test_des = train_test_split(df['description'],df['seed_success'], test_size=0.05, random_state=2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_seed2.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.replace(np.nan, 0, inplace=True)\n",
    "df.replace('NaN', 0, inplace=True)\n",
    "df.replace(' ', 0, inplace=True)\n",
    "\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_d = class_weight.compute_class_weight('balanced', np.unique(y_train_des), y_train_des)\n",
    "class_weight_d = dict(enumerate(class_weight_d))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.fit_on_texts(text_train_des)\n",
    "X_train_des = tokenizer.texts_to_sequences(text_train_des)\n",
    "X_test_des = tokenizer.texts_to_sequences(text_test_des)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPooling1D,Input,Concatenate\n",
    "from keras import Model\n",
    "def cnn_classifier(tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "    maxlen = 100 # or fixed length for improved efficiency\n",
    "    embedding_dim = 300\n",
    "    print('vocabubary size:',vocab_size)\n",
    "    print('max length text:',maxlen)\n",
    " \n",
    "    X_train_des = pad_sequences(X_train_des, padding='post', maxlen=maxlen)\n",
    "    X_test_des = pad_sequences(X_test_des, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_weights = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    for word,index in word_index.items():\n",
    "         embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(embedding_dim)\n",
    "    print(embedding_weights.shape)\n",
    "    nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_weights, axis=1))\n",
    "    print(nonzero_elements / vocab_size) \n",
    "\n",
    "# Deep CNN\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_weights],trainable=False))\n",
    "    model.add(Conv1D(128, 7, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(256, 5, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(512, 3, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)\n",
    "\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [3,5,7]\n",
    "\n",
    "    for fsz in filter_sizes:\n",
    "        x = Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "        x = MaxPooling1D()(x)\n",
    "        convs.append(x)\n",
    "    \n",
    "    x = Concatenate(axis=-1)(convs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, output)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "    history = model.fit(X_train_des, y_train_des,\n",
    "                    class_weight = class_weights_d,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_des, y_test_des),\n",
    "                    batch_size=50)\n",
    "    loss, accuracy = model.evaluate(X_train_des, y_train_des, verbose=True)\n",
    "    return  history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f0be135104611efcd15ce68b6b8a0718351908cecd2bbe264002f4ebdc7d1de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
