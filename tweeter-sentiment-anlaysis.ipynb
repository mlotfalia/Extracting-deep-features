{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip3 install --upgrade tensorflow-gpu --user\n",
    "\n",
    "#!pip install genism\n",
    "#!pip install --upgrade gensim # to upgrade version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "text_train_des, text_test_des, y_train_des, y_test_des = train_test_split(df['description'],df['seed_success'], test_size=0.05, random_state=2)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler, OneHotEncoder,LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "import tensorflow as tf\n",
    "import pandas_profiling as pp\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(dataframe, nominal):\n",
    "    gen_onehot_features = pd.get_dummies(dataframe[nominal])\n",
    "    result = pd.concat([dataframe, gen_onehot_features],axis=1)\n",
    "    result.drop([nominal],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_seed2.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.replace(np.nan, 0, inplace=True)\n",
    "df.replace('NaN', 0, inplace=True)\n",
    "df.replace(' ', 0, inplace=True)\n",
    "\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "      if df.loc[index,'seed_success'] != 0:\n",
    "            if  df.loc[index,'Interval_month_seed_seriesA'] <= 24:\n",
    "                   df.loc[index,'seed_success'] =1\n",
    "         \n",
    "          \n",
    "                    \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df =df[df.announced_year_seed >2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df.replace(np.nan, 0, inplace=True)\n",
    "df.replace('NaN', 0, inplace=True)\n",
    "df.replace(' ', 0, inplace=True)\n",
    "\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['crunchbase_id']= pd.to_numeric(df.crunchbase_id, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['announced_year_seed']).sum().plot(kind='pie', y='seed_success',autopct='%1.0f%%',legend=False ,startangle=0,shadow =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list= df.columns.tolist()\n",
    "print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ \"description\"]=df[ \"description\"].astype('string')\n",
    "df[ 'short_description']=df[ 'short_description'].astype('string')\n",
    "df['techrunch_news_1']=df['techrunch_news_1'].astype('string')\n",
    "df[ 'techrunch_news_2']=df[ 'techrunch_news_2'].astype('string')\n",
    "\n",
    "df['tweet_1.1']=df['tweet_1.1'].astype('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"$\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"%\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"#\", \"\")\n",
    "    \n",
    "      \n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, \"description\")\n",
    "df = standardize_text(df, \"short_description\")\n",
    "df = standardize_text(df, \"techrunch_news_1\")\n",
    "df = standardize_text(df, \"techrunch_news_2\")\n",
    "\n",
    "df = standardize_text(df, \"tweet_1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "df.corr()[\"seed_success\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['num_employees_enum'] = pd.to_numeric(df.num_employees_enum, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['num_articles'] = pd.to_numeric(df.num_articles, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['num_event_appearances'] = pd.to_numeric(df.num_event_appearances, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['num_funding_rounds'] = pd.to_numeric(df.num_funding_rounds, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['facebook'] = pd.to_numeric(df.facebook, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['linkedin'] = pd.to_numeric(df.linkedin, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['twitter'] = pd.to_numeric(df.linkedin, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['announced_year_seed'] = pd.to_numeric(df.announced_year_seed , errors='coerce').fillna(0).astype(np.int64)\n",
    "df['money_raised_seed'] = pd.to_numeric(df.money_raised_seed, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['num_investors_seed'] = pd.to_numeric(df.num_investors_seed, errors='coerce').fillna(0).astype(np.int64)\n",
    "df['announced_year_a'] = pd.to_numeric(df.announced_year_a, errors='coerce').fillna(0).astype(np.int64)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = api.load(\"word2vec-google-news-300\", return_path=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"C:\\\\Users\\\\MAHSA/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "#word2vec = model(word2vec_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, df, generate_missing=False):\n",
    "    embeddings = df['tokens_des'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_d = class_weight.compute_class_weight('balanced', np.unique(y_train_des), y_train_des)\n",
    "class_weight_d = dict(enumerate(class_weight_d))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.fit_on_texts(text_train_des)\n",
    "X_train_des = tokenizer.texts_to_sequences(text_train_des)\n",
    "X_test_des = tokenizer.texts_to_sequences(text_test_des)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers import InputLayer, Conv1D, Dense, Flatten, MaxPooling1D,Input,Concatenate\n",
    "from keras import Model\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "maxlen = 100 # or fixed length for improved efficiency\n",
    "embedding_dim = 300\n",
    "print('vocabubary size:',vocab_size)\n",
    "print('max length text:',maxlen)\n",
    "\n",
    "X_train_des = pad_sequences(X_train_des, padding='post', maxlen=maxlen)\n",
    "X_test_des = pad_sequences(X_test_des, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_weights = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word,index in word_index.items():\n",
    "         embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(embedding_dim)\n",
    "print(embedding_weights.shape)\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_weights, axis=1))\n",
    "print(nonzero_elements / vocab_size) \n",
    "\n",
    "# Deep CNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_weights],trainable=False))\n",
    "model.add(Conv1D(128, 7, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(256, 5, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(512, 3, activation='relu',padding='same'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Kim Yoon CNN\n",
    "model.save('model.h5')\n",
    "model_final = load_model('model.h5')\n",
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_weights],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=False)\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "convs = []\n",
    "filter_sizes = [3,5,7]\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    x = Conv1D(128, fsz, activation='relu',padding='same')(embedded_sequences)\n",
    "    x = MaxPooling1D()(x)\n",
    "    convs.append(x)\n",
    "    \n",
    "x = Concatenate(axis=-1)(convs)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Fit model\n",
    "history = model.fit(X_train_des, y_train_des,\n",
    "                    class_weight = class_weights_d,\n",
    "                    epochs=3,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_des, y_test_des),\n",
    "                    batch_size=50)\n",
    "loss, accuracy = model.evaluate(X_train_des, y_train_des, verbose=True)\n",
    "\n",
    "#model.history\n",
    "y = df[ 'seed_success']  \n",
    "x=df.drop( 'seed_success', axis=1)\n",
    "x_train['cnn_des']=model.predict(X_train_des)\n",
    "x_test['cnn_des']= model.predict(X_test_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weightsd = class_weight.compute_class_weight('balanced', np.unique(y_train_sdes), y_train_sdes)\n",
    "class_weights_sd = dict(enumerate(class_weightsd))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer.fit_on_texts(text_train_sdes)\n",
    "X_train_sdes = tokenizer.texts_to_sequences(text_train_sdes)\n",
    "X_test_sdes = tokenizer.texts_to_sequences(text_test_sdes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
